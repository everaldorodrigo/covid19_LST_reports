{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "from pandas import read_csv\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from io import StringIO\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse PDFs for initial dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create curatedBy Object\n",
    "def generate_curator():\n",
    "    todate = datetime.now()\n",
    "    curatedByObject = {\"@type\": \"Organization\", \"identifier\": \"covid19LST\", \"url\": \"https://www.covid19lst.org/\", \n",
    "                              \"name\": \"COVID-19 Literature Surveillance Team\", \"affiliation\": [], \n",
    "                              \"curationDate\": todate.strftime(\"%Y-%m-%d\")}\n",
    "    return(curatedByObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_author():\n",
    "    authorObject = generate_curator()\n",
    "    authorObject.pop('curationDate')\n",
    "    memberlist = read_csv('data/LST members.txt',delimiter='\\t',header=0,encoding='UTF-8')\n",
    "    memberlist.rename(columns={'affiliation':'affiliation list'}, inplace=True)\n",
    "    memberlist['affiliation']='blank'\n",
    "    for i in range(len(memberlist)):\n",
    "        affiliationlist = memberlist.iloc[i]['affiliation list'].split(';')\n",
    "        tmplist = []\n",
    "        for eachaffiliation in affiliationlist:\n",
    "            tmplist.append({\"name\":eachaffiliation})\n",
    "        memberlist.at[i,'affiliation'] = tmplist\n",
    "    memberlist.drop(columns='affiliation list',inplace=True)\n",
    "    memberdictlist = memberlist.to_dict('records')\n",
    "    authorObject['members']=memberdictlist \n",
    "    return(authorObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abstract(publist):\n",
    "    cleanlist = [str(item) for item in publist]\n",
    "    separator = ', '\n",
    "    abstract = \"Analytical reviews on the level of evidence presented in publications. This report specifically covers the following publications: \"+ separator.join(cleanlist)\n",
    "    return(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Batch convert DOIs\n",
    "def convert_dois(doilist):\n",
    "    doistring = '\"' + '\",\"'.join(doilist) + '\"'\n",
    "    r = requests.post(\"https://api.outbreak.info/resources/query/\", params = {'q': doistring, 'scopes': 'doi', 'fields': '_id,name,url,doi'})\n",
    "    if r.status_code == 200:\n",
    "        rawresult = pandas.read_json(r.text)\n",
    "        if 'notfound' in rawresult.columns:\n",
    "            check = rawresult.loc[(rawresult['notfound']==1.0)|(rawresult['notfound']==True)]\n",
    "            if len(check)==len(doilist):\n",
    "                cleanresult = pandas.DataFrame(columns=['_id','name','url','doi'])\n",
    "                missing = doilist            \n",
    "            else:\n",
    "                no_dups = rawresult[rawresult['query']==rawresult['doi']]\n",
    "                cleanresult = no_dups[['_id','name','url','doi']].loc[~no_dups['_id'].isin(check['_id'].tolist())].copy()\n",
    "                missing = [x for x in doilist if x not in cleanresult['doi'].unique().tolist()]        \n",
    "        else:\n",
    "            no_dups = rawresult[rawresult['query']==rawresult['doi']]\n",
    "            cleanresult = no_dups[['_id','name','url','doi']]\n",
    "            missing = []\n",
    "        cleanresult.drop('doi',axis=1,inplace=True)\n",
    "        \n",
    "    else:\n",
    "        cleanresult=[]\n",
    "        missing=[]\n",
    "    return(cleanresult, missing)\n",
    "\n",
    "### Convert a single doi\n",
    "##\"https://api.outbreak.info/resources/query?q=doi:\"+doi\n",
    "\n",
    "### Batch fetch pmid meta\n",
    "def get_pmid_meta(pmidlist):\n",
    "    pmidstring = '\"' + '\",\"'.join(pmidlist) + '\"'\n",
    "    r = requests.post(\"https://api.outbreak.info/resources/query/\", params = {'q': pmidstring, 'scopes': '_id', 'fields': '_id,name,url'})\n",
    "    if r.status_code == 200:\n",
    "        rawresult = pandas.read_json(r.text)\n",
    "        no_dups = rawresult[rawresult['query']==rawresult['_id']]\n",
    "        if 'notfound' in rawresult.columns:\n",
    "            check = rawresult.loc[(rawresult['notfound']==1.0)|(rawresult['notfound']==True)]\n",
    "            if len(check)==len(pmidlist):\n",
    "                cleanresult = pandas.DataFrame(columns=['_id','name','url'])\n",
    "                missing = pmidlist            \n",
    "            else:\n",
    "                cleanresult = no_dups[['_id','name','url']].loc[~no_dups['_id'].isin(check['_id'].tolist())].copy()\n",
    "                missing = [x for x in pmidlist if x not in cleanresult['_id'].unique().tolist()]\n",
    "        else:\n",
    "            cleanresult = no_dups[['_id','name','url']]\n",
    "            missing = []\n",
    "        \n",
    "    else:\n",
    "        cleanresult=[]\n",
    "        missing=[]\n",
    "    return(cleanresult, missing)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_ids_from_text(output_text):\n",
    "    #### The COVID-19 LST reports do not list pmids in a parsable fashion in the text. Only DOI's can be parsed out\n",
    "    pmidlist = []\n",
    "    doilist = []\n",
    "    check = output_text.split('\\n')\n",
    "    doilines = [x for x in check if 'doi' in x.lower()]\n",
    "    if len(doilines)>0:\n",
    "        for doiline in doilines:\n",
    "            if '\\t' in doiline:\n",
    "                doistart = doiline[doiline.find('doi'):]\n",
    "                doi = doistart[doistart.find('\\t'):doistart.find('.\\t')]\n",
    "                doilist.append(doi.strip())\n",
    "            else:\n",
    "                doistart = doiline[doiline.find('doi'):]\n",
    "                doi = doistart[doistart.find(' '):doistart.find('. ')]\n",
    "                doilist.append(doi.strip())\n",
    "    return(pmidlist,doilist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_urls(eachurl,pmidlist,doilist):\n",
    "    if 'pubmed' in eachurl and '?' not in eachurl:\n",
    "        pmid = eachurl.replace(\"https://www.ncbi.nlm.nih.gov/pubmed/\",\"\").replace(\"https://pubmed.ncbi.nlm.nih.gov/\",\"\").rstrip(\"/\")\n",
    "        if \"#affiliation\" in pmid:\n",
    "            trupmid = pmid.split(\"/\")[0]\n",
    "            tmpid = 'pmid'+trupmid\n",
    "        else:\n",
    "            tmpid = 'pmid'+pmid\n",
    "        pmidlist.append(tmpid)\n",
    "    elif 'doi' in eachurl:\n",
    "        tenplace = eachurl.find('10.')\n",
    "        doi = eachurl[tenplace:]\n",
    "        doilist.append(doi)  \n",
    "    return(pmidlist,doilist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf(eachfile):\n",
    "    pdffile = open('data/reports/'+eachfile,'rb')\n",
    "    parser = PDFParser(pdffile)\n",
    "    doc = PDFDocument(parser)\n",
    "    allurls = []\n",
    "    pmidlist = []\n",
    "    doilist = []\n",
    "    for page in PDFPage.create_pages(doc):\n",
    "        try: \n",
    "            for annotation in page.annots:\n",
    "                annotationDict = annotation.resolve()\n",
    "                if \"A\" in annotationDict:\n",
    "                    uri = annotationDict[\"A\"][\"URI\"].decode('UTF-8').replace(\" \", \"%20\")\n",
    "                    allurls.append(uri)\n",
    "        except:\n",
    "            continue  \n",
    "    if len(allurls)>0:  \n",
    "        for eachurl in allurls:\n",
    "            pmidlist,doilist = parse_urls(eachurl,pmidlist,doilist)              \n",
    "    if len(allurls)==0: \n",
    "        output_string = StringIO()\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        for page in PDFPage.create_pages(doc):\n",
    "            interpreter.process_page(page)\n",
    "        output_text = output_string.getvalue()\n",
    "        pmidlist, doilist = strip_ids_from_text(output_text)  \n",
    "        \n",
    "    pmidlist = list(set(pmidlist))\n",
    "    doilist = list(set(doilist))\n",
    "                   \n",
    "    return(pmidlist,doilist)\n",
    "\n",
    "                           \n",
    "def merge_meta(pmidlist,doilist):\n",
    "    if len(doilist)>0:\n",
    "        doianns,missing_dois = convert_dois(doilist)\n",
    "        doicheck = True\n",
    "    else:\n",
    "        doicheck = False\n",
    "        missing_dois = None\n",
    "    if len(pmidlist)>0:\n",
    "        pmidanns,missing_pmids = get_pmid_meta(pmidlist)\n",
    "        pmidcheck = True\n",
    "    else:\n",
    "        pmidcheck = False\n",
    "        missing_pmids = None\n",
    "    if doicheck==True and pmidcheck==True:\n",
    "        basedOndf = pandas.concat((pmidanns,doianns),ignore_index=True)\n",
    "    elif doicheck==True and pmidcheck==False:\n",
    "        basedOndf = doianns\n",
    "    elif doicheck==False and pmidcheck==True:\n",
    "        basedOndf = pmidanns     \n",
    "    if missing_pmids!=None and missing_dois!=None:\n",
    "        missing = list(set(missing_pmids).union(set(missing_dois)))\n",
    "    elif missing_pmids==None and missing_dois!=None:\n",
    "        missing = missing_dois\n",
    "    elif missing_pmids!=None and missing_dois==None:\n",
    "        missing = missing_pmids\n",
    "    else:\n",
    "        missing = None\n",
    "    return(basedOndf,missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_missing(missing):\n",
    "    try:\n",
    "        missing_list = pickle.load(open('results/pubs_not_yet_in_outbreak.txt','rb'))\n",
    "        if missing != None:\n",
    "            total_missing = list(set([*missing_list, *missing]))\n",
    "            with open('results/pubs_not_yet_in_outbreak.txt','wb') as dmpfile:\n",
    "                pickle.dump(total_missing,dmpfile)\n",
    "    except:\n",
    "        if missing != None:\n",
    "            with open('results/pubs_not_yet_in_outbreak.txt','wb') as dmpfile:\n",
    "                pickle.dump(missing,dmpfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that strftime(\"%d\") will give the day with a leading zero\n",
    "## In windows, strftime(\"%#d\") will give it without leading zeros\n",
    "## In linux, strftime(\"%-d\") will give it without leading zeros\n",
    "def generate_report_url(datePublished):\n",
    "    urlbase = \"https://www.covid19lst.org/post/\"\n",
    "    urlend = \"daily-covid-19-lst-report\"\n",
    "    is_windows = sys.platform.startswith('win')\n",
    "    if is_windows==True:\n",
    "        reporturl = urlbase+datePublished.strftime(\"%B\").lower()+\"-\"+datePublished.strftime(\"%#d\")+\"-\"+urlend\n",
    "    else:\n",
    "        reporturl = urlbase+datePublished.strftime(\"%B\").lower()+\"-\"+datePublished.strftime(\"%-d\")+\"-\"+urlend\n",
    "    return(reporturl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report_meta(filelist):\n",
    "    report_pmid_df = pandas.DataFrame(columns=['_id','name','identifier','url'])\n",
    "    curatedByObject = generate_curator()\n",
    "    author = generate_author()\n",
    "    badpdfs = []\n",
    "    for eachfile in filelist:\n",
    "        reportdate = eachfile[0:4]+'.'+eachfile[4:6]+'.'+eachfile[6:8]\n",
    "        datePublished = datetime.fromisoformat(eachfile[0:4]+'-'+eachfile[4:6]+'-'+eachfile[6:8])\n",
    "        name = \"Covid-19 LST Report \"+reportdate\n",
    "        reporturl = generate_report_url(datePublished)\n",
    "        report_id = 'lst'+reportdate\n",
    "        pmidlist,doilist = parse_pdf(eachfile)\n",
    "        if len(pmidlist)+len(doilist)==0:\n",
    "            badpdfs.append(eachfile)\n",
    "        else:\n",
    "            basedOndf,missing = merge_meta(pmidlist,doilist)\n",
    "            basedOndf['@type']='Publication'\n",
    "            reportlinkdf = basedOndf[['_id','url']]\n",
    "            reportlinkdf['identifier']=report_id\n",
    "            reportlinkdf['url']=reporturl\n",
    "            reportlinkdf['name']=name\n",
    "            report_pmid_df = pandas.concat(([report_pmid_df,reportlinkdf]),ignore_index=True)\n",
    "            report_pmid_df.drop_duplicates(keep='first',inplace=True)\n",
    "            report_pmid_df.to_csv('data/report_pmid_df.txt',sep='\\t',header=True)\n",
    "            save_missing(missing)\n",
    "            abstract = generate_abstract(basedOndf['_id'].unique().tolist())\n",
    "            metadict = {\"@context\": {\"schema\": \"http://schema.org/\", \"outbreak\": \"https://discovery.biothings.io/view/outbreak/\"}, \n",
    "                        \"@type\": \"Publication\", \"journalName\": \"COVID-19 LST Daily Summary Reports\", \"journalNameAbbreviation\": \"covid19LST\", \n",
    "                        \"publicationType\": \"Review\", \"license\":\"(CC BY-NC-SA 4.0) (http://creativecommons.org/licenses/by-nc-sa/4.0/)\",\n",
    "                        \"_id\":report_id,\"curatedBy\": curatedByObject,\"abstract\": abstract, \"name\": name, \n",
    "                        \"datePublished\": datePublished.strftime(\"%Y-%m-%d\"),\"url\": reporturl,\"author\":[author], \n",
    "                        \"isBasedOn\":basedOndf.to_dict('records')}\n",
    "            yield(metadict)\n",
    "        except:\n",
    "            save_missing(list(report_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unit test\n",
    "\n",
    "dumpdir = 'data/reports/'\n",
    "filelist = os.listdir(dumpdir)\n",
    "testlist = filelist[0:2]\n",
    "metadict = generate_report_meta(testlist)\n",
    "for eachdict in metadict:\n",
    "    print(eachdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download PDFs from google drive\n",
    "\n",
    "https://drive.google.com/drive/folders/1603ahBNdt1SnSaYYBE-G8SA6qgRTQ6fF\n",
    "\n",
    "but only if they were uploaded after a specific date. The reason for this is that prior to 2020.09.11, there was variability in the file naming and file names were sometimes incorrect (202080 instead of 202008). These have been manually corrected in the initial dump, but are otherwise still incorrect in the google drive.\n",
    "\n",
    "Note that this script uses the googledrive api which requires authentification even when accessing a public google drive. To fulfill this requirement without needing to manually log in, credentials from a service account are needed  The googledrive API is only used to read the files in the drive so that the newest ones (by date) can be identified, and their id's taken.\n",
    "\n",
    "Additionally the the pydrive2 library (use to access the google drive api) sometimes has trouble finding the client_secrets.json file, so you may need to manually point to it.\n",
    "\n",
    "The downloader uses the GoogleDriveDownloader library which is based off of requests and should not require the google drive api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function identifies files uploaded after 2020.09.11 that have NOT yet been downloaded\n",
    "## Note that this is the function if a service account is not available. It requires a login\n",
    "\n",
    "def check_google():\n",
    "    from pydrive2.auth import GoogleAuth\n",
    "    from pydrive2.drive import GoogleDrive\n",
    "\n",
    "    GoogleAuth.DEFAULT_SETTINGS['client_config_file'] = 'client_secrets.json' ##point to secrets file location\n",
    "    gauth = GoogleAuth()\n",
    "    #gauth.LocalWebserverAuth()\n",
    "\n",
    "    drive = GoogleDrive(gauth)\n",
    "    file_id = '1603ahBNdt1SnSaYYBE-G8SA6qgRTQ6fF'\n",
    "    file_list = drive.ListFile({'q': \"'%s' in parents and trashed=false\" % file_id}).GetList()\n",
    "    \n",
    "    df = pandas.DataFrame(file_list)\n",
    "    dfclean = df[['createdDate','id','title']].copy()\n",
    "    dfclean['date'] = pandas.to_datetime(dfclean['createdDate'],format='%Y-%m-%d', errors='coerce')\n",
    "    lastupdate = dfclean.loc[dfclean['createdDate']=='2020-09-11T01:53:29.639Z'].iloc[0]['date']\n",
    "    dfnew = dfclean.loc[dfclean['date']>lastupdate]\n",
    "    \n",
    "    all_files = os.listdir('data/reports/')\n",
    "    new_files = [item for item in dfnew['title'].unique().tolist() if item not in all_files]\n",
    "    reportdf = dfnew.loc[dfnew['title'].isin(new_files)]\n",
    "    return(reportdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function identifies files uploaded after 2020.09.11 that have NOT yet been downloaded\n",
    "## Note that this is the function if a service account IS available. \n",
    "def check_google():\n",
    "    from pydrive2.auth import GoogleAuth\n",
    "    from pydrive2.drive import GoogleDrive\n",
    "    from pydrive2.auth import ServiceAccountCredentials\n",
    "    \n",
    "    gauth = GoogleAuth()\n",
    "    scope = ['https://www.googleapis.com/auth/drive']\n",
    "    gauth.credentials = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)\n",
    "    drive = GoogleDrive(gauth)\n",
    "    file_id = '1603ahBNdt1SnSaYYBE-G8SA6qgRTQ6fF'\n",
    "    file_list = drive.ListFile({'q': \"'%s' in parents and trashed=false\" % file_id}).GetList()\n",
    "    \n",
    "    df = pandas.DataFrame(file_list)\n",
    "    dfclean = df[['createdDate','id','title']].copy()\n",
    "    dfclean['date'] = pandas.to_datetime(dfclean['createdDate'],format='%Y-%m-%d', errors='coerce')\n",
    "    lastupdate = dfclean.loc[dfclean['createdDate']=='2020-09-11T01:53:29.639Z'].iloc[0]['date']\n",
    "    dfnew = dfclean.loc[dfclean['date']>lastupdate]\n",
    "    \n",
    "    all_files = os.listdir('data/reports/')\n",
    "    new_files = [item for item in dfnew['title'].unique().tolist() if item not in all_files]\n",
    "    reportdf = dfnew.loc[dfnew['title'].isin(new_files)]\n",
    "    return(reportdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This is the function to actually conduct the download\n",
    "## Note that the report date in the title is used for generating the corresponding covid19 LST report url\n",
    "## For this reason, special reports cannot be automated and will be exempt from downloading\n",
    "def download_reports(reportdf):\n",
    "    from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "    notdownloaded = 0\n",
    "    for i in range(len(reportdf)):\n",
    "        title = reportdf.iloc[i]['title']\n",
    "        eachid = reportdf.iloc[i]['id']\n",
    "        try:\n",
    "            date_title = int(title[0:6])\n",
    "            gdd.download_file_from_google_drive(file_id=eachid,\n",
    "                                                dest_path='data/reports/'+title,\n",
    "                                                unzip=False)\n",
    "        except:\n",
    "            notdownloaded = notdownloaded+1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations():\n",
    "    reportdf = check_google()\n",
    "    download_reports(reportdf)\n",
    "    dumpdir = 'data/reports/'\n",
    "    filelist = os.listdir(dumpdir)\n",
    "    metadict = generate_report_meta(filelist)\n",
    "    yield from(metadict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Unit test\n",
    "\n",
    "#reportdf = check_google()\n",
    "#download_reports(reportdf)\n",
    "dumpdir = 'data/reports/'\n",
    "filelist = os.listdir(dumpdir)\n",
    "#metadict = generate_report_meta(filelist)\n",
    "#for eachmeta in metadict:\n",
    "#    print(eachmeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filelist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time:  2021-02-26 13:33:58.419988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gtsueng\\Anaconda3\\envs\\outbreak\\lib\\site-packages\\pandas\\core\\frame.py:3990: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "end time:  2021-02-26 13:40:51.126061\n"
     ]
    }
   ],
   "source": [
    "##### Unit test\n",
    "##https://www.covid19lst.org/post/september-11-daily-covid-19-lst-report\n",
    "#dt.strftime(\"%A, %d. %B %Y %I:%M%p\")\n",
    "#'Tuesday, 21. November 2006 04:30PM'\n",
    "\n",
    "report_pmid_df = pandas.DataFrame(columns=['_id','name','identifier','url'])\n",
    "curatedByObject = generate_curator()\n",
    "author = curatedByObject.copy()\n",
    "author.pop(\"curationDate\")\n",
    "#curatedByObject.pop('members')\n",
    "print('start time: ',datetime.now())\n",
    "badpdfs = []\n",
    "\n",
    "for eachfile in filelist:\n",
    "    reportdate = eachfile[0:4]+'.'+eachfile[4:6]+'.'+eachfile[6:8]\n",
    "    datePublished = datetime.fromisoformat(eachfile[0:4]+'-'+eachfile[4:6]+'-'+eachfile[6:8])\n",
    "    name = \"Covid-19 LST Report \"+reportdate\n",
    "    reporturl = generate_report_url(datePublished)\n",
    "    report_id = 'lst'+reportdate\n",
    "    pmidlist,doilist = parse_pdf(eachfile)\n",
    "    if len(pmidlist)+len(doilist)==0:\n",
    "        badpdfs.append(eachfile)\n",
    "    else:\n",
    "        basedOndf,missing = merge_meta(pmidlist,doilist)\n",
    "        basedOndf['@type']='Publication'\n",
    "        reportlinkdf = basedOndf[['_id','url']].copy()\n",
    "        reportlinkdf['identifier']=report_id\n",
    "        reportlinkdf['url']=reporturl\n",
    "        reportlinkdf['name']=name\n",
    "        report_pmid_df = pandas.concat(([report_pmid_df,reportlinkdf]),ignore_index=True)\n",
    "        report_pmid_df.drop_duplicates(keep='first',inplace=True)\n",
    "        report_pmid_df.to_csv('data/report_pmid_df.txt',sep='\\t',header=True)\n",
    "        save_missing(missing)\n",
    "        abstract = generate_abstract(basedOndf['_id'].unique().tolist())\n",
    "        metadict = {\"@context\": {\"schema\": \"http://schema.org/\", \"outbreak\": \"https://discovery.biothings.io/view/outbreak/\"}, \n",
    "                    \"@type\": \"Publication\", \"journalName\": \"COVID-19 LST Daily Summary Reports\", \"journalNameAbbreviation\": \"covid19LST\", \n",
    "                    \"publicationType\": \"Review\", \"license\":\"(CC BY-NC-SA 4.0) (http://creativecommons.org/licenses/by-nc-sa/4.0/)\",\n",
    "                    \"_id\":report_id,\"curatedBy\": curatedByObject,\"abstract\": abstract, \"name\": name, \n",
    "                    \"datePublished\": datePublished.strftime(\"%Y-%m-%d\"),\"url\": reporturl, \n",
    "                    \"isBasedOn\":basedOndf.to_dict('records')}\n",
    "        with open(\"results/pdfminer2/\"+name+\".json\", \"w\") as outfile:  \n",
    "            json.dump(metadict, outfile) \n",
    "\n",
    "print(len(badpdfs))    \n",
    "print('end time: ',datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(badpdfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filelist[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "\n",
    "\n",
    "with open(dumpdir+filelist[30], 'rb') as pdffile:\n",
    "    parser = PDFParser(pdffile)\n",
    "    doc = PDFDocument(parser)\n",
    "    allurls = []\n",
    "    pmidlist = []\n",
    "    doilist = []\n",
    "    for page in PDFPage.create_pages(doc):\n",
    "        try: \n",
    "            for annotation in page.annots:\n",
    "                annotationDict = annotation.resolve()\n",
    "                if \"A\" in annotationDict:\n",
    "                    uri = annotationDict[\"A\"][\"URI\"].decode('UTF-8').replace(\" \", \"%20\")\n",
    "                    allurls.append(uri)\n",
    "        except:\n",
    "            continue\n",
    "    if len(allurls)>0:  \n",
    "        for eachurl in allurls:\n",
    "            pmidlist,doilist = parse_urls(eachurl,pmidlist,doilist)              \n",
    "    if len(allurls)==0: \n",
    "        output_string = StringIO()\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        for page in PDFPage.create_pages(doc):\n",
    "            interpreter.process_page(page)\n",
    "        output_text = output_string.getvalue()\n",
    "        print(output_text)\n",
    "        #pmidlist, doilist = strip_ids_from_text(output_text)  \n",
    "\n",
    "#print(doilist)                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmidlist = []\n",
    "doilist = []\n",
    "check = output_text.split('\\n')\n",
    "doilines = [x for x in check if 'doi' in x.lower()]\n",
    "\n",
    "if len(doilines)>0:\n",
    "    for doiline in doilines:\n",
    "        if '\\t' in doiline:\n",
    "            doistart = doiline[doiline.find('doi'):]\n",
    "            doi = doistart[doistart.find('\\t'):doistart.find('.\\t')]\n",
    "            doilist.append(doi.strip())\n",
    "        else:\n",
    "            doistart = doiline[doiline.find('doi'):]\n",
    "            doi = doistart[doistart.find(' '):doistart.find('. ')]\n",
    "            doilist.append(doi.strip())\n",
    "\n",
    "print(doilist)\n",
    "#print(doilist)\n",
    "#doistring = '\"' + '\",\"'.join(doilist) + '\"'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = result.split('\\n')\n",
    "dois = [x for x in check if 'doi' in x.lower()]\n",
    "pmids = [x for x in check if ('pmid' or 'pubmed') in x.lower()]\n",
    "doiline = dois[4]\n",
    "doistart = doiline[doiline.find('doi'):]\n",
    "doi = doistart[doistart.find('\\t'):doistart.find('.\\t')]\n",
    "print(doi.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doistring = '\"' + '\",\"'.join(doilist) + '\"'\n",
    "r = requests.post(\"https://api.outbreak.info/resources/query/\", params = {'q': doistring, 'scopes': 'doi', 'fields': '_id,name,url,doi'})\n",
    "print(r.status_code)\n",
    "\n",
    "\"\"\"if r.status_code == 200:\n",
    "    rawresult = pandas.read_json(r.text)\n",
    "    if 'notfound' in rawresult.columns:\n",
    "        check = rawresult.loc[(rawresult['notfound']==1.0)|(rawresult['notfound']==True)]\n",
    "        if len(check)==len(doilist):\n",
    "            cleanresult = pandas.DataFrame(columns=['_id','name','url','doi'])\n",
    "            missing = doilist            \n",
    "        else:\n",
    "            no_dups = rawresult[rawresult['query']==rawresult['doi']]\n",
    "            cleanresult = no_dups[['_id','name','url','doi']].loc[~no_dups['_id'].isin(check['_id'].tolist())].copy()\n",
    "            missing = [x for x in doilist if x not in cleanresult['doi'].unique().tolist()]        \n",
    "    else:\n",
    "        no_dups = rawresult[rawresult['query']==rawresult['doi']]\n",
    "        cleanresult = no_dups[['_id','name','url','doi']]\n",
    "        missing = []\n",
    "    cleanresult.drop('doi',axis=1,inplace=True)\n",
    "\n",
    "else:\n",
    "    cleanresult=[]\n",
    "    missing=[]\"\"\"\n",
    "\n",
    "print(doistring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank = StringIO()\n",
    "a='Howdy!'\n",
    "b='How ya doing?'\n",
    "c=\"what's happening man?\"\n",
    "x=\"Whatchu up to?\"\n",
    "melist = [a,b,c,x]\n",
    "for eachelement in melist:\n",
    "    blank.write(eachelement)\n",
    "result = blank.getvalue()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
