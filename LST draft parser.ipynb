{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "from pandas import read_csv\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import PyPDF2 as pypdf\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the data dump\n",
    "\n",
    "Note that this code still needs a downloader to pull data from either github or google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create curatedBy Object\n",
    "def generate_curator():\n",
    "    todate = datetime.now()\n",
    "    curatedByObject = {\"@type\": \"Organization\", \"identifier\": \"covid19LST\", \"url\": \"https://www.covid19lst.org/\", \n",
    "                              \"name\": \"COVID-19 Literature Surveillance Team\", \"affiliation\": \"\", \n",
    "                              \"curationDate\": todate.strftime(\"%Y-%m-%d\")}\n",
    "    return(curatedByObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_author():\n",
    "    authorObject = generate_curator()\n",
    "    authorObject.pop('curationDate')\n",
    "    memberlist = read_csv('data/LST members.txt',delimiter='\\t',header=0,encoding='UTF-8')\n",
    "    memberlist.rename(columns={'affiliation':'affiliation list'}, inplace=True)\n",
    "    memberlist['affiliation']='blank'\n",
    "    for i in range(len(memberlist)):\n",
    "        affiliationlist = memberlist.iloc[i]['affiliation list'].split(';')\n",
    "        tmplist = []\n",
    "        for eachaffiliation in affiliationlist:\n",
    "            tmplist.append({\"name\":eachaffiliation})\n",
    "        memberlist.at[i,'affiliation'] = tmplist\n",
    "    memberlist.drop(columns='affiliation list',inplace=True)\n",
    "    memberdictlist = memberlist.to_dict('records')\n",
    "    authorObject['members']=memberdictlist \n",
    "    return(authorObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_keywords(keywordstring):\n",
    "    if keywordstring != keywordstring: ## Is it Nan?\n",
    "        keywordlist = []\n",
    "    elif keywordstring ==\"\": ## Is it an empty string?\n",
    "        keywordlist = []\n",
    "    elif keywordstring == None: ## Is there no keywordstring?\n",
    "        keywordlist = []\n",
    "    else:\n",
    "        keywordlist = keywordstring.lstrip('[').rstrip(']').replace('\"','').split(',')\n",
    "    return(keywordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lst_dump(datadmp):\n",
    "    cleandata = []\n",
    "    authorObject = generate_author()\n",
    "    authorObject.pop('members')\n",
    "    datadmp['_id'] = 'pmid'+datadmp['PMID'].astype(str)  \n",
    "    for i in range(len(datadmp)):\n",
    "        keywordlist = fix_keywords(datadmp.iloc[i]['Topics'])\n",
    "        tmpdict={'_id':datadmp.iloc[i]['_id'],'keywords':keywordlist,\n",
    "                 'covid19LST':{'@type':'Rating',\n",
    "                                 'ratingExplanation':datadmp.iloc[i]['Methodology'],\n",
    "                                 'ratingValue':datadmp.iloc[i]['LevelOfEvidence'],\n",
    "                                 'reviewAspect':'Oxford 2011 Levels of Evidence',\n",
    "                                 'author':authorObject}}\n",
    "        cleandata.append(tmpdict)\n",
    "    return(cleandata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_filelist():\n",
    "    all_files = os.listdir('data/tables/')\n",
    "    updatefiles = all_files.remove('COVID-19 LST Data.csv')\n",
    "    initial_file = 'data/tables/COVID-19 LST Data.csv'\n",
    "    df = read_csv(initial_file,header=0,usecols=['PMID','Topics','LevelOfEvidence','Methodology','Updated Date'])\n",
    "    if updatefiles!=None:\n",
    "        for eachfile in updatefiles:\n",
    "            tmpfile = read_csv('data/tables/'+eachfile,header=0,usecols=['PMID','Topics','LevelOfEvidence','Methodology','Updated Date'])\n",
    "            df = pandas.concat((df,tmpfile),ignore_index=True)\n",
    "    else:\n",
    "        nochange=True\n",
    "    df.sort_values('Updated Date',ascending=False,inplace=True)\n",
    "    df.drop_duplicates(subset='PMID',keep='first',inplace=True)\n",
    "    return(df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run an update\n",
    "\n",
    "def run_loe_update():\n",
    "    datadmp = update_filelist()\n",
    "    dictlist = generate_lst_dump(datadmp)\n",
    "    with open('results/update_dumps/litcovid_lst.json', 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(dictlist, indent=4))\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse PDFs for initial dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abstract(publist):\n",
    "    separator = ', '\n",
    "    abstract = \"Analytical reviews on the level of evidence presented in publications. This report specifically covers the following publications: \"+ separator.join(publist)\n",
    "    return(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Batch convert DOIs\n",
    "def convert_dois(doilist):\n",
    "    doistring = '\"' + '\",\"'.join(doilist) + '\"'\n",
    "    r = requests.post(\"https://api.outbreak.info/resources/query/\", params = {'q': doistring, 'scopes': 'doi', 'fields': '_id,name,url,doi'})\n",
    "    if r.status_code == 200:\n",
    "        rawresult = pandas.read_json(r.text)\n",
    "        cleanresult = rawresult.drop_duplicates(subset='_id',keep=\"first\")\n",
    "        if len(doilist)>len(cleanresult):\n",
    "            missing = [x for x in doilist if x not in cleanresult['doi'].unique().tolist()]\n",
    "        else:\n",
    "            missing = None\n",
    "        cleanresult.drop(columns=['doi','_score','query'],inplace=True)\n",
    "    return(cleanresult, missing)\n",
    "\n",
    "### Convert a single doi\n",
    "##\"https://api.outbreak.info/resources/query?q=doi:\"+doi\n",
    "\n",
    "### Batch fetch pmid meta\n",
    "def get_pmid_meta(pmidlist):\n",
    "    pmidstring = '\"' + '\",\"'.join(pmidlist) + '\"'\n",
    "    r = requests.post(\"https://api.outbreak.info/resources/query/\", params = {'q': pmidstring, 'scopes': '_id', 'fields': '_id,name,url'})\n",
    "    if r.status_code == 200:\n",
    "        rawresult = pandas.read_json(r.text)\n",
    "        cleanresult = rawresult[['_id','name','url']].loc[rawresult['_score']==1].copy()\n",
    "        cleanresult.drop_duplicates(subset='_id',keep=\"first\", inplace=True)\n",
    "        if len(pmidlist)>len(cleanresult):\n",
    "            missing = [x for x in pmidlist if x not in cleanresult['_id'].unique().tolist()]\n",
    "        else:\n",
    "            missing = None\n",
    "    return(cleanresult, missing)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf(eachfile):\n",
    "    pdffile = open('data/reports/'+eachfile,'rb')\n",
    "    pdf = pypdf.PdfFileReader(pdffile)\n",
    "    pages = pdf.getNumPages()\n",
    "    key = '/Annots'\n",
    "    uri = '/URI'\n",
    "    ank = '/A'\n",
    "    allurls = []\n",
    "    for page in range(pages):\n",
    "        pageSliced = pdf.getPage(page)\n",
    "        pageObject = pageSliced.getObject()\n",
    "        if key in pageObject:\n",
    "            ann = pageObject[key]\n",
    "            for a in ann:\n",
    "                u = a.getObject()\n",
    "                if ank in u:\n",
    "                    if uri in u[ank]:\n",
    "                        allurls.append(u[ank][uri])\n",
    "    pmidlist = []\n",
    "    doilist = []\n",
    "    for eachurl in allurls:\n",
    "        if 'pubmed' in eachurl and '?' not in eachurl:\n",
    "            pmid = eachurl.replace(\"https://www.ncbi.nlm.nih.gov/pubmed/\",\"\").replace(\"https://pubmed.ncbi.nlm.nih.gov/\",\"\").rstrip(\"/\")\n",
    "            if \"#affiliation\" in pmid:\n",
    "                trupmid = pmid.split(\"/\")[0]\n",
    "                tmpid = 'pmid'+trupmid\n",
    "            else:\n",
    "                tmpid = 'pmid'+pmid\n",
    "            pmidlist.append(tmpid)\n",
    "        elif 'doi' in eachurl:\n",
    "            doi = eachurl.replace(\"https://doi.org/\",\"\").rstrip(\"/\")\n",
    "            doilist.append(doi)\n",
    "        pmidlist = list(set(pmidlist))\n",
    "        doilist = list(set(doilist))\n",
    "    return(pmidlist,doilist)\n",
    "\n",
    "def merge_meta(pmidlist,doilist):\n",
    "    if len(doilist)>0:\n",
    "        doianns,missing_dois = convert_dois(doilist)\n",
    "        doicheck = True\n",
    "    else:\n",
    "        doicheck = False\n",
    "        missing_dois = None\n",
    "    if len(pmidlist)>0:\n",
    "        pmidanns,missing_pmids = get_pmid_meta(pmidlist)\n",
    "        pmidcheck = True\n",
    "    else:\n",
    "        pmidcheck = False\n",
    "        missing_pmids = None\n",
    "    if doicheck==True and pmidcheck==True:\n",
    "        basedOndf = pandas.concat((pmidanns,doianns),ignore_index=True)\n",
    "    elif doicheck==True and pmidcheck==False:\n",
    "        basedOndf = doianns\n",
    "    elif doicheck==False and pmidcheck==True:\n",
    "        basedOndf = pmidanns     \n",
    "    if missing_pmids!=None and missing_dois!=None:\n",
    "        missing = list(set(missing_pmids).union(set(missing_dois)))\n",
    "    elif missing_pmids==None and missing_dois!=None:\n",
    "        missing = missing_dois\n",
    "    elif missing_pmids!=None and missing_dois==None:\n",
    "        missing = missing_pmids\n",
    "    else:\n",
    "        missing = None\n",
    "    return(basedOndf,missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_missing(missing):\n",
    "    try:\n",
    "        missing_list = pickle.load(open('results/pubs_not_yet_in_outbreak.txt','rb'))\n",
    "        if missing != None:\n",
    "            total_missing = list(set([*missing_list, *missing]))\n",
    "            with open('results/pubs_not_yet_in_outbreak.txt','wb') as dmpfile:\n",
    "                pickle.dump(total_missing,dmpfile)\n",
    "    except:\n",
    "        if missing != None:\n",
    "            with open('results/pubs_not_yet_in_outbreak.txt','wb') as dmpfile:\n",
    "                pickle.dump(total_missing,dmpfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generated_citedBy_dict():\n",
    "    txtdmp = read_csv('results/update dumps/litcovid_citedBy.tsv', delimiter='\\t', header=0, index_col=0)\n",
    "    dictlist = []\n",
    "    for i in range(len(txtdump)):\n",
    "        tmpdict={'_id':txtdump.iloc[i]['_id'],'citedBy':[{'@type':'Publication',\n",
    "                                                                'identifier':txtdump.iloc[i]['identifier'],\n",
    "                                                                'name':txtdump.iloc[i]['name'],\n",
    "                                                                'url':txtdump.iloc[i]['url']}]}\n",
    "        dictlist.append(tmpdict)\n",
    "    with open('results/update dumps/litcovid_citedBy.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(dictlist, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that strftime(\"%d\") will give the day with a leading zero\n",
    "## In windows, strftime(\"%#d\") will give it without leading zeros\n",
    "## In linux, strftime(\"%-d\") will give it without leading zeros\n",
    "def generate_report_url(datePublished):\n",
    "    urlbase = \"https://www.covid19lst.org/post/\"\n",
    "    urlend = \"daily-covid-19-lst-report\"\n",
    "    is_windows = sys.platform.startswith('win')\n",
    "    if is_windows==True:\n",
    "        reporturl = urlbase+datePublished.strftime(\"%B\").lower()+\"-\"+datePublished.strftime(\"%#d\")+\"-\"+urlend\n",
    "    else:\n",
    "        reporturl = urlbase+datePublished.strftime(\"%B\").lower()+\"-\"+datePublished.strftime(\"%-d\")+\"-\"+urlend\n",
    "    return(reporturl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report_meta(filelist):\n",
    "    report_pmid_df = pandas.DataFrame(columns=['_id','name','identifier','url'])\n",
    "    curatedByObject = generate_curator()\n",
    "    author = generate_author()\n",
    "    for eachfile in filelist:\n",
    "        reportdate = eachfile[0:4]+'.'+eachfile[4:6]+'.'+eachfile[6:8]\n",
    "        datePublished = datetime.fromisoformat(eachfile[0:4]+'-'+eachfile[4:6]+'-'+eachfile[6:8])\n",
    "        name = \"Covid-19 LST Report \"+reportdate\n",
    "        reporturl = generate_report_url(datePublished)\n",
    "        report_id = 'lst'+reportdate\n",
    "        pmidlist,doilist = parse_pdf(eachfile)\n",
    "        basedOndf,missing = merge_meta(pmidlist,doilist)\n",
    "        reportlinkdf = basedOndf[['_id','url']]\n",
    "        reportlinkdf['identifier']=report_id\n",
    "        reportlinkdf['url']=reporturl\n",
    "        reportlinkdf['name']=name\n",
    "        report_pmid_df = pandas.concat(([report_pmid_df,reportlinkdf]),ignore_index=True)\n",
    "        report_pmid_df.drop_duplicates(keep='first',inplace=True)\n",
    "        report_pmid_df.to_csv('data/report_pmid_df.txt',sep='\\t',header=True)\n",
    "        save_missing(missing)\n",
    "        abstract = generate_abstract(basedOndf['_id'].unique().tolist())\n",
    "        metadict = {\"@context\": {\"schema\": \"http://schema.org/\", \"outbreak\": \"https://discovery.biothings.io/view/outbreak/\"}, \n",
    "                    \"@type\": \"Publication\", \"journalName\": \"COVID-19 LST Daily Summary Reports\", \"journalNameAbbreviation\": \"covid19LST\", \n",
    "                    \"publicationType\": \"Review\", \"license\":\"(CC BY-NC-SA 4.0) (http://creativecommons.org/licenses/by-nc-sa/4.0/)\",\n",
    "                    \"_id\":report_id,\"curatedBy\": curatedByObject,\"abstract\": abstract, \"name\": name, \n",
    "                    \"datePublished\": datePublished.strftime(\"%y-%m-%d\"),\"url\": reporturl,\"author\":[author], \n",
    "                    \"isBasedOn\":basedOndf.to_dict('records')}\n",
    "        yield(metadict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumpdir = 'data/reports/'\n",
    "filelist = os.listdir(dumpdir)\n",
    "testlist = filelist[0:2]\n",
    "metadict = generate_report_meta(testlist)\n",
    "for eachdict in metadict:\n",
    "    print(eachdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download PDFs from google drive\n",
    "\n",
    "https://drive.google.com/drive/folders/1603ahBNdt1SnSaYYBE-G8SA6qgRTQ6fF\n",
    "\n",
    "but only if they were uploaded after a specific date. The reason for this is that prior to 2020.09.11, there was variability in the file naming and file names were sometimes incorrect (202080 instead of 202008). These have been manually corrected in the initial dump, but are otherwise still incorrect in the google drive.\n",
    "\n",
    "Note that this script uses the googledrive api which requires authentification even when accessing a public google drive. To fulfill this requirement without needing to manually log in, credentials from a service account are needed  The googledrive API is only used to read the files in the drive so that the newest ones (by date) can be identified, and their id's taken.\n",
    "\n",
    "Additionally the the pydrive2 library (use to access the google drive api) sometimes has trouble finding the client_secrets.json file, so you may need to manually point to it.\n",
    "\n",
    "The downloader uses the GoogleDriveDownloader library which is based off of requests and should not require the google drive api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function identifies files uploaded after 2020.09.11 that have NOT yet been downloaded\n",
    "## Note that this is the function if a service account is not available. It requires a login\n",
    "\n",
    "def check_google():\n",
    "    from pydrive2.auth import GoogleAuth\n",
    "    from pydrive2.drive import GoogleDrive\n",
    "\n",
    "    GoogleAuth.DEFAULT_SETTINGS['client_config_file'] = 'client_secrets.json' ##point to secrets file location\n",
    "    gauth = GoogleAuth()\n",
    "    #gauth.LocalWebserverAuth()\n",
    "\n",
    "    drive = GoogleDrive(gauth)\n",
    "    file_id = '1603ahBNdt1SnSaYYBE-G8SA6qgRTQ6fF'\n",
    "    file_list = drive.ListFile({'q': \"'%s' in parents and trashed=false\" % file_id}).GetList()\n",
    "    \n",
    "    df = pandas.DataFrame(file_list)\n",
    "    dfclean = df[['createdDate','id','title']].copy()\n",
    "    dfclean['date'] = pandas.to_datetime(dfclean['createdDate'],format='%Y-%m-%d', errors='coerce')\n",
    "    lastupdate = dfclean.loc[dfclean['createdDate']=='2020-09-11T01:53:29.639Z'].iloc[0]['date']\n",
    "    dfnew = dfclean.loc[dfclean['date']>lastupdate]\n",
    "    \n",
    "    all_files = os.listdir('data/reports/')\n",
    "    new_files = [item  for item in all_files if item not in dfnew['title'].unique().tolist()]\n",
    "    reportdf = dfnew.loc[dfnew['title'].isin(new_files)]\n",
    "    return(reportdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         kind                                 id                  etag  \\\n",
      "0  drive#file  1EyccnGO2UOun668cv4ddE6kg4jggm31J  \"MTYwMDQ0OTE5MTAwMA\"   \n",
      "1  drive#file  1fxpNQRGhg8UNBIzSEBDKD0m-DjSB-1fE  \"MTYwMDM5NDk3MDAwMA\"   \n",
      "\n",
      "                                            selfLink  \\\n",
      "0  https://www.googleapis.com/drive/v2/files/1Eyc...   \n",
      "1  https://www.googleapis.com/drive/v2/files/1fxp...   \n",
      "\n",
      "                                      webContentLink  \\\n",
      "0  https://drive.google.com/uc?id=1EyccnGO2UOun66...   \n",
      "1  https://drive.google.com/uc?id=1fxpNQRGhg8UNBI...   \n",
      "\n",
      "                                       alternateLink  \\\n",
      "0  https://drive.google.com/file/d/1EyccnGO2UOun6...   \n",
      "1  https://drive.google.com/file/d/1fxpNQRGhg8UNB...   \n",
      "\n",
      "                                           embedLink  \\\n",
      "0  https://drive.google.com/file/d/1EyccnGO2UOun6...   \n",
      "1  https://drive.google.com/file/d/1fxpNQRGhg8UNB...   \n",
      "\n",
      "                                            iconLink  \\\n",
      "0  https://drive-thirdparty.googleusercontent.com...   \n",
      "1  https://drive-thirdparty.googleusercontent.com...   \n",
      "\n",
      "                                       thumbnailLink  \\\n",
      "0  https://lh3.googleusercontent.com/bgcvSn-sQnnH...   \n",
      "1  https://lh3.googleusercontent.com/HW2RXRkoSq2F...   \n",
      "\n",
      "                                    title  ...  \\\n",
      "0  20200918 The COVID-19 Daily Report.pdf  ...   \n",
      "1  20200917 The COVID-19 Daily Report.pdf  ...   \n",
      "\n",
      "                          capabilities editable  copyable writersCanShare  \\\n",
      "0  {'canCopy': True, 'canEdit': False}    False      True            True   \n",
      "1  {'canCopy': True, 'canEdit': False}    False      True            True   \n",
      "\n",
      "  shared explicitlyTrashed appDataContents  \\\n",
      "0   True             False           False   \n",
      "1   True             False           False   \n",
      "\n",
      "                                      headRevisionId   spaces exportLinks  \n",
      "0  0Bz4ifkSdU_1dU3FJZGFuN0xKRml6YWl4TEZUN3YvU0pZW...  [drive]         NaN  \n",
      "1  0B0FA6-NbD0WZVUVnWlFFNUhseFhXQjNFcU5LVXVFNnJFL...  [drive]         NaN  \n",
      "\n",
      "[2 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "## This function identifies files uploaded after 2020.09.11 that have NOT yet been downloaded\n",
    "## Note that this is the function if a service account IS available. \n",
    "def check_google():\n",
    "    from pydrive2.auth import GoogleAuth\n",
    "    from pydrive2.drive import GoogleDrive\n",
    "    from pydrive2.auth import ServiceAccountCredentials\n",
    "    \n",
    "    gauth = GoogleAuth()\n",
    "    scope = ['https://www.googleapis.com/auth/drive']\n",
    "    gauth.credentials = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)\n",
    "    drive = GoogleDrive(gauth)\n",
    "    file_id = '1603ahBNdt1SnSaYYBE-G8SA6qgRTQ6fF'\n",
    "    file_list = drive.ListFile({'q': \"'%s' in parents and trashed=false\" % file_id}).GetList()\n",
    "    \n",
    "    df = pandas.DataFrame(file_list)\n",
    "    dfclean = df[['createdDate','id','title']].copy()\n",
    "    dfclean['date'] = pandas.to_datetime(dfclean['createdDate'],format='%Y-%m-%d', errors='coerce')\n",
    "    lastupdate = dfclean.loc[dfclean['createdDate']=='2020-09-11T01:53:29.639Z'].iloc[0]['date']\n",
    "    dfnew = dfclean.loc[dfclean['date']>lastupdate]\n",
    "    \n",
    "    all_files = os.listdir('data/reports/')\n",
    "    new_files = [item  for item in all_files if item not in dfnew['title'].unique().tolist()]\n",
    "    reportdf = dfnew.loc[dfnew['title'].isin(new_files)]\n",
    "    return(reportdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the function to actually conduct the download\n",
    "def download_reports(reportdf):\n",
    "    from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "    for i in range(len(dfnew)):\n",
    "        title = reportdf.iloc[i]['title']\n",
    "        eachid = reportdf.iloc[i]['id']\n",
    "        gdd.download_file_from_google_drive(file_id=eachid,\n",
    "                                            dest_path='data/reports/'+title,\n",
    "                                            unzip=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the functions\n",
    "reportdf = check_google()\n",
    "download_reports(reportdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download CSV data updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load list of files that have already been run/loaded\n",
    "def load_prior_runs():\n",
    "    prior_runs = pickle.load(open('results/prior_runs.txt','rb'))\n",
    "    return(prior_runs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Checks folder for new data dump to parse against list that have already been run/loaded\n",
    "def update_check():\n",
    "    prior_runs = load_prior_runs()\n",
    "    all_files = os.listdir('data/reports/')\n",
    "    new_files = [item  for item in all_files if item not in prior_runs]\n",
    "    return(new_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generates list of PMIDs to update with COVID-19 LST info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### parse into json with covid-19 LST info to add to existing pmid data and dump to results folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order of operations\n",
    "1. Generate the reports\n",
    "2. Generate the citedBy dump to be appended\n",
    "3. Generate the litcovid extra value info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Unit test\n",
    "##https://www.covid19lst.org/post/september-11-daily-covid-19-lst-report\n",
    "#dt.strftime(\"%A, %d. %B %Y %I:%M%p\")\n",
    "#'Tuesday, 21. November 2006 04:30PM'\n",
    "\n",
    "report_pmid_df = pandas.DataFrame(columns=['_id','name','identifier','url'])\n",
    "curatedByObject = generate_curator()\n",
    "author = curatedByObject.copy()\n",
    "author.pop(\"curationDate\")\n",
    "curatedByObject.pop('members')\n",
    "\n",
    "for eachfile in filelist[103:104]:\n",
    "    reportdate = eachfile[0:4]+'.'+eachfile[4:6]+'.'+eachfile[6:8]\n",
    "    datePublished = datetime.fromisoformat(eachfile[0:4]+'-'+eachfile[4:6]+'-'+eachfile[6:8])\n",
    "    name = \"Covid-19 LST Report \"+reportdate\n",
    "    reporturl = generate_report_url(datePublished)\n",
    "    report_id = 'lst'+reportdate\n",
    "    pmidlist,doilist = parse_pdf(eachfile)\n",
    "    basedOndf,missing = merge_meta(pmidlist,doilist)\n",
    "    reportlinkdf = basedOndf[['_id','url']]\n",
    "    reportlinkdf['identifier']=report_id\n",
    "    reportlinkdf['url']=reporturl\n",
    "    reportlinkdf['name']=name\n",
    "    report_pmid_df = pandas.concat(([report_pmid_df,reportlinkdf]),ignore_index=True)\n",
    "    report_pmid_df.drop_duplicates(keep='first',inplace=True)\n",
    "    report_pmid_df.to_csv('data/report_pmid_df.txt',sep='\\t',header=True)\n",
    "    save_missing(missing)\n",
    "    abstract = generate_abstract(basedOndf['_id'].unique().tolist())\n",
    "    metadict = {\"@context\": {\"schema\": \"http://schema.org/\", \"outbreak\": \"https://discovery.biothings.io/view/outbreak/\"}, \n",
    "                \"@type\": \"Publication\", \"journalName\": \"COVID-19 LST Daily Summary Reports\", \"journalNameAbbreviation\": \"covid19LST\", \n",
    "                \"publicationType\": \"Review\", \"license\":\"(CC BY-NC-SA 4.0) (http://creativecommons.org/licenses/by-nc-sa/4.0/)\",\n",
    "                \"_id\":report_id,\"curatedBy\": curatedByObject,\"abstract\": abstract, \"name\": name, \n",
    "                \"datePublished\": datePublished.strftime(\"%y-%m-%d\"),\"url\": reporturl, \n",
    "                \"isBasedOn\":basedOndf.to_dict('records')}\n",
    "    print(metadict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
